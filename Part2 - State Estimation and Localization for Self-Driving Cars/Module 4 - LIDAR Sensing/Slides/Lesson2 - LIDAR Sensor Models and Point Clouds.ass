WEBVTT

1
00:00:00.000 --> 00:00:10.000
[MUSIC]

2
00:00:14.530 --> 00:00:18.757
In the last video, we talked about
the basic operating principles of LIDAR,

3
00:00:18.757 --> 00:00:22.860
one of the most popular sensor choices for
self-driving cars.

4
00:00:22.860 --> 00:00:27.463
In the next two videos we'll learn how we
can use the point cloud generated by LIDAR

5
00:00:27.463 --> 00:00:30.813
sensors to do state estimation for
our self-driving car.

6
00:00:30.813 --> 00:00:35.350
By the end of this video, you'll be able
to describe the basic point cloud data

7
00:00:35.350 --> 00:00:37.569
structure used to store LIDAR scans.

8
00:00:39.080 --> 00:00:44.343
Describe common spatial operations on
point clouds such as rotation and scaling.

9
00:00:44.343 --> 00:00:48.319
And use the method of least squares to
fit a plane to a point cloud in order to

10
00:00:48.319 --> 00:00:50.260
detect the road or other surfaces.

11
00:00:52.410 --> 00:00:57.300
To start us off, recall that a 3D LIDAR
sensor returns measurements of range,

12
00:00:57.300 --> 00:01:01.740
elevation angle, and azimuth angle for
every point that it scans.

13
00:01:01.740 --> 00:01:05.200
We know how to convert these spherical
coordinates into Cartesian x, y,

14
00:01:05.200 --> 00:01:08.650
zed coordinates using
the inverse sensor model s o

15
00:01:08.650 --> 00:01:12.620
we can build up a large point cloud using
all the measurements from a LIDAR scan.

16
00:01:13.930 --> 00:01:16.460
For some LIDAR setups,
it's not uncommon for

17
00:01:16.460 --> 00:01:19.329
these point clouds to contain
upwards of a million points or more.

18
00:01:21.210 --> 00:01:23.300
So what can we do these
massive point clouds?

19
00:01:24.500 --> 00:01:27.840
Let's consider an example of a point cloud
we might encounter in the real world.

20
00:01:29.090 --> 00:01:32.960
Let's say our LIDAR scans a nearby
tree off on the side of the road, and

21
00:01:32.960 --> 00:01:35.070
produces a point cloud
that looks like this.

22
00:01:36.590 --> 00:01:40.350
We only see points on the part of the tree
that's facing us because the tree and

23
00:01:40.350 --> 00:01:41.990
the leaves reflect infrared light.

24
00:01:43.500 --> 00:01:48.160
The first question you might ask is how
do we keep track of all of these points?

25
00:01:48.160 --> 00:01:50.590
What kinds of data structures
should we use to work with them?

26
00:01:52.690 --> 00:01:57.513
One common solution is to assign
an index to each of the points,

27
00:01:57.513 --> 00:02:01.335
say point 1 through point n,
and store the x, y,

28
00:02:01.335 --> 00:02:05.705
zed coordinates of each point
as a 3 by 1 column vector.

29
00:02:05.705 --> 00:02:10.350
From there, you could think about storing
each of these vectors in a list, or

30
00:02:10.350 --> 00:02:14.509
you could stack them side by side
into a matrix that we'll call big P.

31
00:02:14.509 --> 00:02:18.353
Doing it this way make it easier to
work with the standard linear algebra

32
00:02:18.353 --> 00:02:22.709
libraries, like the Python NumPy library,
which lets us take advantage of fast

33
00:02:22.709 --> 00:02:25.721
matrix operations rather than
iterating over a list and

34
00:02:25.721 --> 00:02:27.860
treating each vector independently.

35
00:02:28.960 --> 00:02:31.330
So what kind of operations
are we talking about?

36
00:02:31.330 --> 00:02:34.499
There are three basic spatial
operations that are important for

37
00:02:34.499 --> 00:02:37.033
carrying out state estimation
with point clouds.

38
00:02:39.159 --> 00:02:43.200
Translation, rotation, and scaling.

39
00:02:43.200 --> 00:02:44.999
We'll talk about each of these in turn.

40
00:02:47.074 --> 00:02:50.113
When we think about spatial
operations on point clouds,

41
00:02:50.113 --> 00:02:54.547
our intuition might be to think in terms
of physically manipulating the point cloud

42
00:02:54.547 --> 00:02:56.715
while our reference frame stays fixed.

43
00:02:57.891 --> 00:02:59.127
But for state estimation,

44
00:02:59.127 --> 00:03:01.929
it's more useful to think about
things the other way around.

45
00:03:03.265 --> 00:03:06.529
Objects in the world mostly stay
put while the reference frame

46
00:03:06.529 --> 00:03:11.000
attached to the vehicle moves and observes
the world from different perspectives.

47
00:03:12.888 --> 00:03:16.680
So let's think about how translating our
reference frame, say, by driving for

48
00:03:16.680 --> 00:03:20.529
a ten meters will affect our perception
of a single point in point cloud.

49
00:03:22.050 --> 00:03:25.020
We can start by drawing the vector
from the origin of our sensor frame,

50
00:03:25.020 --> 00:03:26.250
S, to a point, P.

51
00:03:27.770 --> 00:03:30.510
Now, consider a second frame, S-prime,

52
00:03:30.510 --> 00:03:34.320
whose origin has been translated relative
to S due to motion of the vehicle.

53
00:03:35.860 --> 00:03:40.540
Note that the basis vectors of frame
S-prime are the same as the basis vectors

54
00:03:40.540 --> 00:03:41.510
of frame S.

55
00:03:41.510 --> 00:03:42.830
Only the origin has moved.

56
00:03:44.380 --> 00:03:47.690
We can draw another vector from
the origin of S-prime to the point P.

57
00:03:48.810 --> 00:03:51.342
Immediately, we notice
the resulting vector,

58
00:03:51.342 --> 00:03:54.932
indicated here, is just the tip to
tail sum of the other two vectors.

59
00:03:56.852 --> 00:04:00.656
And these vectors are just geometric
objects until we express them in

60
00:04:00.656 --> 00:04:01.909
a coordinate system.

61
00:04:01.909 --> 00:04:07.348
And what we're after are the coordinates
of the point P in frame S-prime.

62
00:04:07.348 --> 00:04:11.777
We can get these easily by just
subtracting the frame-to-frame

63
00:04:11.777 --> 00:04:15.653
translation vector from
the coordinates of P in frame S.

64
00:04:15.653 --> 00:04:20.451
This extends easily to a batch operation
on the full point cloud by simply tiling

65
00:04:20.451 --> 00:04:23.890
the frame-to-frame translation
in a big matrix R, and

66
00:04:23.890 --> 00:04:26.630
subtracting it from
the point cloud matrix.

67
00:04:27.940 --> 00:04:31.580
Depending on the language or
linear algebra library you're using,

68
00:04:31.580 --> 00:04:34.870
you probably won't need to
build this R matrix explicitly.

69
00:04:34.870 --> 00:04:38.550
In Python, for example, the NumPy
library is smart enough to repeat

70
00:04:38.550 --> 00:04:42.873
the frame-to-frame translation
implicitly using broadcasting semantics.

71
00:04:44.347 --> 00:04:47.730
Now, let's think about what happens if
rotate our reference frame instead of

72
00:04:47.730 --> 00:04:48.505
translating it.

73
00:04:50.371 --> 00:04:54.076
Again, keep in mind that we're not
changing the physical point P,

74
00:04:54.076 --> 00:04:55.940
only our view of it.

75
00:04:55.940 --> 00:04:56.800
So in this case,

76
00:04:56.800 --> 00:05:00.852
we only have to think about one vector
from the origin of frame S to P.

77
00:05:02.290 --> 00:05:06.840
What does change in this case is actually
the set of basis vectors we use to

78
00:05:06.840 --> 00:05:09.690
express the coordinates
of the vector S to P.

79
00:05:11.516 --> 00:05:16.013
Remember that the rotation matrix C tells
us how to find the coordinates of a vector

80
00:05:16.013 --> 00:05:20.205
in a rotated frame from the coordinates
of the vector in the original frame.

81
00:05:22.270 --> 00:05:26.843
So if we know the rotation matrix
from frame S to frame S-prime,

82
00:05:26.843 --> 00:05:31.669
all we have to do is multiply it
against the coordinates of P in frame S

83
00:05:31.669 --> 00:05:34.994
to get the coordinates
of P in frame S-prime.

84
00:05:34.994 --> 00:05:38.903
To determine the coordinates of the entire
rotated point cloud, the operation is

85
00:05:38.903 --> 00:05:42.370
exactly the same, thanks to
the properties of matrix multiplication.

86
00:05:44.428 --> 00:05:47.330
The last spatial operation
to think about is scaling,

87
00:05:47.330 --> 00:05:50.080
which works very similarly to rotation.

88
00:05:50.080 --> 00:05:52.750
But instead of changing the direction
of the basis vectors in our

89
00:05:52.750 --> 00:05:55.540
coordinate system,
we're changing their lengths.

90
00:05:57.620 --> 00:06:01.430
Mathematically, this just means
pre-multiplying the coordinates of

91
00:06:01.430 --> 00:06:05.950
each point by a diagonal matrix
S whose non-zero elements

92
00:06:05.950 --> 00:06:08.870
are simply the desired scaling
factors along each dimension.

93
00:06:10.190 --> 00:06:13.210
Often but not always these
scaling factors are the same,

94
00:06:13.210 --> 00:06:16.620
and the matrix multiplication is
equivalent to multiplying by a scaler.

95
00:06:18.260 --> 00:06:22.272
In these cases, we say that the scaling
is isotropic or equal in every direction.

96
00:06:24.844 --> 00:06:28.622
We can use the same matrix multiplication
for individual points or for

97
00:06:28.622 --> 00:06:31.707
the entire point cloud,
just like we did for rotations.

98
00:06:33.299 --> 00:06:36.974
Usually, the transformations we're
interested in are a combination of

99
00:06:36.974 --> 00:06:39.529
translation and rotation and
sometimes scaling.

100
00:06:40.730 --> 00:06:43.950
For example, we're often interested
in estimating the translation and

101
00:06:43.950 --> 00:06:47.330
rotation that best aligns
to point clouds so

102
00:06:47.330 --> 00:06:49.820
that we can estimate the motion
of our self-driving car.

103
00:06:51.020 --> 00:06:52.980
We'll talk about how we can
do this in the next video.

104
00:06:54.160 --> 00:06:58.156
Fortunately for us, it's easy to combine
all three operations into a single

105
00:06:58.156 --> 00:07:03.300
equation By first translating each vector,

106
00:07:03.300 --> 00:07:08.340
then rotating into the new frame,
and finally applying any scaling.

107
00:07:10.450 --> 00:07:13.227
And of course, this operation
extends to the batch case as well.

108
00:07:17.064 --> 00:07:21.500
So we've seen how to apply basic
spatial operations to point clouds.

109
00:07:21.500 --> 00:07:25.820
And we'll see in the next video how we can
use these concepts to do state estimation

110
00:07:25.820 --> 00:07:27.205
for self-driving cars.

111
00:07:28.690 --> 00:07:32.381
But before we get there, there's one
more important operation to discuss, and

112
00:07:32.381 --> 00:07:34.500
that's plane fitting.

113
00:07:34.500 --> 00:07:37.722
One of the most common and important
applications of plane-fitting for

114
00:07:37.722 --> 00:07:40.573
self-driving cars is figuring out
where the road surface is and

115
00:07:40.573 --> 00:07:43.501
predicting where it's going to
be as the car continues driving.

116
00:07:46.230 --> 00:07:49.364
If you think back to your
high school geometry classes,

117
00:07:49.364 --> 00:07:52.173
you might remember
the equation of a plane in 3D.

118
00:07:52.173 --> 00:07:56.708
Zed equals a plus bx plus cy.

119
00:07:56.708 --> 00:07:59.589
This equation tells you how
the height of the plane z

120
00:07:59.589 --> 00:08:02.920
changes as you move around in the x and
y directions.

121
00:08:02.920 --> 00:08:07.276
And it depends on three parameters, a,
b, and c, which tells you the slope of

122
00:08:07.276 --> 00:08:11.310
the plane in each direction and
where the zed axis intersects the plane.

123
00:08:13.130 --> 00:08:16.378
So in our case, we have a bunch
of measurements of x, y and

124
00:08:16.378 --> 00:08:20.916
zed from our LIDAR point cloud, and we
want to find values for the parameters a,

125
00:08:20.916 --> 00:08:24.612
b, and c that give us the plane
of best fit through these points.

126
00:08:26.198 --> 00:08:30.272
To do this, we're going to reach back
to Module One for our tool of choice,

127
00:08:30.272 --> 00:08:31.966
least-squares estimation.

128
00:08:33.746 --> 00:08:38.320
We'll start by defining a measurement
error e for each point in the point cloud.

129
00:08:38.320 --> 00:08:41.550
And e is just going to be the difference
between the predicted value

130
00:08:41.550 --> 00:08:45.835
of our dependent variable zed-hat and
the actual observed value of zed.

131
00:08:47.430 --> 00:08:52.064
We get zed-hat simply by plugging our
current guess for the parameters a-hat,

132
00:08:52.064 --> 00:08:55.350
b-hat, and c-hat, and
the actual values of x and y in.

133
00:08:56.830 --> 00:09:01.480
In this case, the error, e, that we are
considering, is for a bumpy road surface,

134
00:09:01.480 --> 00:09:02.570
for example.

135
00:09:02.570 --> 00:09:05.203
That is,
a surface which is not exactly planar.

136
00:09:06.619 --> 00:09:10.366
For the moment, we're ignoring the actual
errors in the LIDAR measurements

137
00:09:10.366 --> 00:09:12.388
themselves, which also have an effect.

138
00:09:14.228 --> 00:09:16.995
We can stack all of these error
terms into matrix form so

139
00:09:16.995 --> 00:09:19.350
we have a big matrix of
coefficients called a.

140
00:09:20.390 --> 00:09:25.350
Multiplied by our parameter vector x,
minus our stack measurements b.

141
00:09:27.290 --> 00:09:30.250
You can work out the matrix
multiplication yourself to see that we get

142
00:09:30.250 --> 00:09:32.959
back the same measurement error
equations we started out with.

143
00:09:34.480 --> 00:09:37.002
Now, all we have to do is minimize
the square of this error and

144
00:09:37.002 --> 00:09:38.143
we'll have our solution.

145
00:09:40.654 --> 00:09:44.603
This works exactly the same way as
the resistor problem we worked through in

146
00:09:44.603 --> 00:09:45.301
module one.

147
00:09:46.942 --> 00:09:50.696
We can start by multiplying out
the square to get a matrix polynomial in

148
00:09:50.696 --> 00:09:52.039
the parameter vector x.

149
00:09:53.070 --> 00:09:56.980
From there, we take the partial derivative
of the squared error function with respect

150
00:09:56.980 --> 00:10:01.580
to the parameter vector x and
set it to 0 to find the minimum.

151
00:10:01.580 --> 00:10:04.800
This gives us the linear system we'll
need to solve to obtain the final

152
00:10:04.800 --> 00:10:05.700
least squares estimate.

153
00:10:06.880 --> 00:10:10.000
We can solve this linear system using
an efficient numerical solver like

154
00:10:10.000 --> 00:10:12.980
Python NumPy's solve function.

155
00:10:12.980 --> 00:10:16.348
Or just use the pseudo inverse to get our
final answer for the plane parameters.

156
00:10:18.403 --> 00:10:21.411
One important thing to notice here
is that we did not account for

157
00:10:21.411 --> 00:10:23.640
sensor noise in our x,
y, zed measurements.

158
00:10:24.760 --> 00:10:28.060
All we've done is to find the plane
of best fit through a set of points.

159
00:10:29.960 --> 00:10:33.690
It's certainly possible to set this
problem up in a more sophisticated way

160
00:10:33.690 --> 00:10:35.470
that does account for sensor noise.

161
00:10:36.890 --> 00:10:40.870
You could use a batch approach
similar to what we just discussed, or

162
00:10:40.870 --> 00:10:44.600
you could even think about including the
road parameters in the column filter to

163
00:10:44.600 --> 00:10:47.070
estimate them on the fly as
the sensor data comes in.

164
00:10:48.300 --> 00:10:49.254
The best solution for

165
00:10:49.254 --> 00:10:53.017
your self-driving application will depend
on how much you trust your LIDAR data and

166
00:10:53.017 --> 00:10:56.100
how much thought you want to give
to uncertainty in the road surface.

167
00:10:58.213 --> 00:11:02.978
Now, although all of the operations we've
described here can be easily implemented

168
00:11:02.978 --> 00:11:05.692
with NumPy or
any other linear algebra library,

169
00:11:05.692 --> 00:11:10.059
there is a fantastic open source tool
called the Point Cloud Library, or PCL,

170
00:11:10.059 --> 00:11:15.320
that provides all sorts of useful
functions for working with point clouds.

171
00:11:15.320 --> 00:11:19.270
In fact, it's so useful that you'll
find it everywhere in industry.

172
00:11:19.270 --> 00:11:21.860
The core library is built with C++, but

173
00:11:21.860 --> 00:11:24.080
there are unofficial Python
bindings available as well.

174
00:11:25.310 --> 00:11:27.635
If you want to learn more
about the features PCL,

175
00:11:27.635 --> 00:11:31.161
I highly recommend visiting
pointclouds.org and having a look around.

176
00:11:33.717 --> 00:11:37.330
So to recap, we've seen that point
clouds are a way of capturing all of

177
00:11:37.330 --> 00:11:39.237
the measurements from a LIDAR scan.

178
00:11:39.237 --> 00:11:41.355
And they are often stored as a big matrix.

179
00:11:42.901 --> 00:11:47.335
We saw how we can use linear algebra to
do useful operations on point clouds,

180
00:11:47.335 --> 00:11:50.051
like translating, rotating, and scaling.

181
00:11:51.941 --> 00:11:55.325
And we also saw how we can use
the least squares algorithm to

182
00:11:55.325 --> 00:11:58.585
fit a 3D plane to a point cloud
to find the road surface.

183
00:12:00.032 --> 00:12:04.129
The Point Cloud Library, or PCL,
implements a bunch of useful tools for

184
00:12:04.129 --> 00:12:06.262
working with point clouds in C++.

185
00:12:06.262 --> 00:12:11.177
One of the most useful algorithms in PCL
is called the iterative closest point

186
00:12:11.177 --> 00:12:14.277
algorithm, or ICP,
which is a common method for

187
00:12:14.277 --> 00:12:19.130
estimating the motion of a self-driving
car using two LIDAR point clouds.

188
00:12:19.130 --> 00:12:21.890
We'll talk about how ICP
works in the next video.

189
00:12:21.890 --> 00:12:31.854
[MUSIC]