WEBVTT

1
00:00:20.090 --> 00:00:23.325
Welcome to module four of the course.

2
00:00:23.325 --> 00:00:26.145
In this module, we'll
be talking about LIDAR,

3
00:00:26.145 --> 00:00:28.590
or light detection and ranging sensors.

4
00:00:28.590 --> 00:00:32.760
LIDAR has been an enabling technology
for self-driving cars because it can

5
00:00:32.760 --> 00:00:35.100
see in all directions and is able to

6
00:00:35.100 --> 00:00:37.755
provide very accurate range information.

7
00:00:37.755 --> 00:00:40.095
In fact, with few exceptions,

8
00:00:40.095 --> 00:00:42.260
most self-driving cars
on the road today are

9
00:00:42.260 --> 00:00:45.245
equipped with some type of LIDAR sensor.

10
00:00:45.245 --> 00:00:47.990
In this module, you'll learn about

11
00:00:47.990 --> 00:00:51.030
the operating principles
of LIDAR sensors,

12
00:00:51.030 --> 00:00:55.999
basic sensor models used to work with
LIDAR data and LIDAR point clouds,

13
00:00:55.999 --> 00:00:59.825
different kinds of transformation
operations applied to point clouds,

14
00:00:59.825 --> 00:01:02.645
and how we can use LIDAR to localize

15
00:01:02.645 --> 00:01:07.470
a self-driving car using a technique
called point cloud registration.

16
00:01:07.480 --> 00:01:10.370
In this video specifically,

17
00:01:10.370 --> 00:01:13.550
we'll explore how a LIDAR works and

18
00:01:13.550 --> 00:01:17.495
take a look at sensor models
for 2D and 3D LIDARs.

19
00:01:17.495 --> 00:01:19.850
We'll also describe the sources of

20
00:01:19.850 --> 00:01:23.080
measurement noise and
errors for these sensors.

21
00:01:23.080 --> 00:01:27.280
In future lessons, we'll discuss
in more detail how we can use

22
00:01:27.280 --> 00:01:31.580
LIDAR data for state estimation
of self-driving vehicles.

23
00:01:32.040 --> 00:01:34.720
If you've ever seen
a self-driving car like

24
00:01:34.720 --> 00:01:36.980
the Waymo vehicle or an Uber car,

25
00:01:36.980 --> 00:01:40.690
you've probably noticed something
spinning on the roof of the car.

26
00:01:40.690 --> 00:01:43.300
That something is a LIDAR,

27
00:01:43.300 --> 00:01:45.700
or light detection and ranging sensor,

28
00:01:45.700 --> 00:01:47.410
and its job is to provide

29
00:01:47.410 --> 00:01:51.170
detailed 3D scans of
the environment around the vehicle.

30
00:01:51.170 --> 00:01:55.180
In fact, LIDAR is one of
the most common sensors used on

31
00:01:55.180 --> 00:01:59.600
self-driving cars and
many other kinds of mobile robots.

32
00:02:00.090 --> 00:02:03.559
LIDARs come in many
different shapes and sizes,

33
00:02:03.559 --> 00:02:06.175
and can measure the
distances to a single point,

34
00:02:06.175 --> 00:02:07.975
a 2D slice of the world,

35
00:02:07.975 --> 00:02:11.030
or perform a full 3D scan.

36
00:02:11.170 --> 00:02:14.540
Some of the most popular
models used today are

37
00:02:14.540 --> 00:02:17.345
manufactured by firms such
as Velodyne in California,

38
00:02:17.345 --> 00:02:20.775
Hokuyo in Japan, and SICK in Germany.

39
00:02:20.775 --> 00:02:23.160
In this video, we'll mainly focus on

40
00:02:23.160 --> 00:02:25.890
the Velodyne sensors as
our example of choice,

41
00:02:25.890 --> 00:02:30.145
but the basic techniques applied
to other types of LIDARs as well.

42
00:02:30.145 --> 00:02:33.950
Before we get into
the nitty-gritty of LIDAR sensing,

43
00:02:33.950 --> 00:02:37.520
let's take a quick look at
the history of this important sensor.

44
00:02:37.520 --> 00:02:41.270
LIDAR was first introduced in the 1960s,

45
00:02:41.270 --> 00:02:44.360
not long after the invention
of the laser itself.

46
00:02:44.360 --> 00:02:47.495
The first group to use
LIDAR were meteorologists

47
00:02:47.495 --> 00:02:50.585
at the US National Center
for Atmospheric Research,

48
00:02:50.585 --> 00:02:54.145
who deployed LIDAR to measure
the height of cloud ceilings.

49
00:02:54.145 --> 00:02:57.110
These ground-based
celiometers are still in

50
00:02:57.110 --> 00:02:59.590
use today not only to
measure water clouds,

51
00:02:59.590 --> 00:03:02.840
but also to detect volcanic ash
and air pollution.

52
00:03:02.840 --> 00:03:05.720
Airborne LIDAR sensors are
commonly used today to

53
00:03:05.720 --> 00:03:08.925
survey and map the earth's
surface for agriculture,

54
00:03:08.925 --> 00:03:12.790
geology, military, and other uses.

55
00:03:12.800 --> 00:03:16.020
But the application that
first brought LIDAR

56
00:03:16.020 --> 00:03:18.925
into the public
consciousness was Apollo 15,

57
00:03:18.925 --> 00:03:21.165
the fourth manned mission
to land on the moon,

58
00:03:21.165 --> 00:03:25.355
and the first to use a laser altimeter
to map the surface of the moon.

59
00:03:25.355 --> 00:03:27.770
So, we've seen that LIDAR can be used to

60
00:03:27.770 --> 00:03:31.160
measure distances and create
a certain type of map,

61
00:03:31.160 --> 00:03:33.275
but how did they actually work,

62
00:03:33.275 --> 00:03:36.815
and how can we use them
onboard a self-driving car?

63
00:03:36.815 --> 00:03:40.340
To build a basic LIDAR in one dimension,

64
00:03:40.340 --> 00:03:42.260
you need three components;

65
00:03:42.260 --> 00:03:44.750
a laser, a photodetector,

66
00:03:44.750 --> 00:03:46.985
and a very precise stopwatch.

67
00:03:46.985 --> 00:03:50.630
The laser first emits
a short pulse of light usually in

68
00:03:50.630 --> 00:03:55.325
the near infrared frequency band
along some known ray direction.

69
00:03:55.325 --> 00:03:58.715
At the same time,
the stopwatch begins counting.

70
00:03:58.715 --> 00:04:01.310
The laser pulse travels
outwards from the sensor at

71
00:04:01.310 --> 00:04:03.830
the speed of light and
hits a distant target.

72
00:04:03.830 --> 00:04:06.590
Maybe another vehicle in
front of us on the road or

73
00:04:06.590 --> 00:04:10.010
a stationary object like
a stop sign or a building.

74
00:04:10.010 --> 00:04:14.150
As long as the surface of the target
isn't too polished or shiny,

75
00:04:14.150 --> 00:04:17.660
the laser pulse will scatter off
the surface in all directions,

76
00:04:17.660 --> 00:04:19.430
and some of that reflected light will

77
00:04:19.430 --> 00:04:22.395
travel back along
the original ray direction.

78
00:04:22.395 --> 00:04:27.260
The photodetector catches that return
pulse and the stopwatch tells you

79
00:04:27.260 --> 00:04:29.360
how much time has passed between when

80
00:04:29.360 --> 00:04:32.255
the pulse first went out
and when it came back.

81
00:04:32.255 --> 00:04:35.665
That time is called the round-trip time.

82
00:04:35.665 --> 00:04:38.220
Now, we know the speed of light,

83
00:04:38.220 --> 00:04:40.850
which is a bit less than 300
million meters per second.

84
00:04:40.850 --> 00:04:44.300
So, we can multiply the speed of
light by the round-trip time to

85
00:04:44.300 --> 00:04:48.390
determine the total round trip distance
traveled by the laser pulse.

86
00:04:48.390 --> 00:04:51.880
Since light travels
much faster than cars,

87
00:04:51.880 --> 00:04:55.700
it's a good approximation to think
of the LIDAR and the target as being

88
00:04:55.700 --> 00:04:58.550
effectively stationary
during the few nanoseconds

89
00:04:58.550 --> 00:05:00.880
that it takes for all of this to happen.

90
00:05:00.880 --> 00:05:04.310
That means that the distance from
the LIDAR to the target is simply

91
00:05:04.310 --> 00:05:07.910
half of the round-trip distance
we just calculated.

92
00:05:07.910 --> 00:05:11.000
This technique is called
time-of-flight ranging.

93
00:05:11.000 --> 00:05:13.820
Although it's not the only way
to build a LIDAR,

94
00:05:13.820 --> 00:05:16.355
it's a very common method
that also gets used with

95
00:05:16.355 --> 00:05:19.880
other types of ranging sensors
like radar and sonar.

96
00:05:19.880 --> 00:05:21.740
It's worth mentioning that

97
00:05:21.740 --> 00:05:24.290
the photodetector also
tells you the intensity of

98
00:05:24.290 --> 00:05:28.655
the return pulse relative to the
intensity of the pulse that was emitted.

99
00:05:28.655 --> 00:05:32.870
This intensity information is less
commonly used for self-driving,

100
00:05:32.870 --> 00:05:35.720
but it provides some extra information
about the geometry of

101
00:05:35.720 --> 00:05:39.810
the environment and the material
the beam is reflecting off of.

102
00:05:39.850 --> 00:05:42.820
So, why is intensity data useful?

103
00:05:42.820 --> 00:05:46.880
Well in part, it turns out that it's
possible to create 2D images from

104
00:05:46.880 --> 00:05:49.970
LIDAR intensity data that
you can then use the same

105
00:05:49.970 --> 00:05:53.470
computer vision algorithms you'll
learn about in the next course.

106
00:05:53.470 --> 00:05:55.810
Since LIDAR is its own light source,

107
00:05:55.810 --> 00:06:00.050
it actually provides a way for
self-driving cars to see in the dark.

108
00:06:00.050 --> 00:06:03.020
So, now we know how to
measure a single distance to

109
00:06:03.020 --> 00:06:06.350
a single point using
a laser, a photodetector,

110
00:06:06.350 --> 00:06:09.360
a stopwatch, and
the time-of-flight equation,

111
00:06:09.360 --> 00:06:11.195
but obviously it's not enough to stay

112
00:06:11.195 --> 00:06:14.255
laser focused on a single point ahead.

113
00:06:14.255 --> 00:06:16.550
So, how do we use
this technique to measure

114
00:06:16.550 --> 00:06:20.305
a whole bunch of
distances in 2D or in 3D?

115
00:06:20.305 --> 00:06:23.990
The trick is to build
a rotating mirror into the LIDAR that

116
00:06:23.990 --> 00:06:27.455
directs the emitted pulses
along different directions.

117
00:06:27.455 --> 00:06:29.285
As the mirror rotates,

118
00:06:29.285 --> 00:06:34.760
you can measure distances at points
in a 2D slice around the sensor.

119
00:06:34.760 --> 00:06:37.940
If you then add an up
and down nodding motion

120
00:06:37.940 --> 00:06:40.415
to the mirror along with the rotation,

121
00:06:40.415 --> 00:06:44.165
you can use the same principle
to create a scan in 3D.

122
00:06:44.165 --> 00:06:46.360
For Velodyne type LIDARs,

123
00:06:46.360 --> 00:06:49.820
where the mirror rotates
along the entire sensor body,

124
00:06:49.820 --> 00:06:53.720
it's much harder to use
a knotting motion to make a 3D scan.

125
00:06:53.720 --> 00:06:56.900
Instead, these sensors
will actually create

126
00:06:56.900 --> 00:06:59.690
multiple 2D scan lines from a series of

127
00:06:59.690 --> 00:07:03.320
individual lasers spaced at
fixed angular intervals,

128
00:07:03.320 --> 00:07:05.240
which effectively lets you paint

129
00:07:05.240 --> 00:07:08.550
the world with horizontal
stripes of laser light.

130
00:07:08.830 --> 00:07:12.530
Here's an example of
a typical raw LIDAR stream

131
00:07:12.530 --> 00:07:16.175
from a Velodyne sensor
attached to the roof of a car.

132
00:07:16.175 --> 00:07:18.170
The black hole in the middle is

133
00:07:18.170 --> 00:07:20.860
a blind spot where the sensor
itself is located,

134
00:07:20.860 --> 00:07:23.630
and the concentric circles
spreading outward from there are

135
00:07:23.630 --> 00:07:28.280
the individual scan lines produced
by the rotating Velodyne sensor.

136
00:07:28.280 --> 00:07:33.380
Each point in the scan is colored by
the intensity of the return signal.

137
00:07:33.380 --> 00:07:38.180
The entire collection of points in
the 3D scan is called a point cloud,

138
00:07:38.180 --> 00:07:40.340
and we'll be talking about
how to use point clouds for

139
00:07:40.340 --> 00:07:43.795
state estimation in the next few videos.

140
00:07:43.795 --> 00:07:46.425
But before we get to point clouds,

141
00:07:46.425 --> 00:07:49.625
we need to think about
individual points in 3D.

142
00:07:49.625 --> 00:07:52.730
Now typically, LIDARs
measure the position

143
00:07:52.730 --> 00:07:55.570
of points in 3D using
spherical coordinates,

144
00:07:55.570 --> 00:08:00.020
range or radial distance from
the center origin to the 3D point,

145
00:08:00.020 --> 00:08:04.370
elevation angle measured up
from the sensors XY plane,

146
00:08:04.370 --> 00:08:09.755
and azimuth angle, measured
counterclockwise from the sensors x-axis.

147
00:08:09.755 --> 00:08:12.650
This makes sense because
the azimuth and elevation

148
00:08:12.650 --> 00:08:15.440
angles tell you the direction
of the laser pulse,

149
00:08:15.440 --> 00:08:17.120
and the range tells you how far in

150
00:08:17.120 --> 00:08:19.880
that direction
the target point is located.

151
00:08:19.880 --> 00:08:22.790
The azimuth and elevation angles
are measured using

152
00:08:22.790 --> 00:08:25.400
encoders that tell you
the orientation of the mirror,

153
00:08:25.400 --> 00:08:29.390
and the range is measured using the time
of flight as we've seen before.

154
00:08:29.390 --> 00:08:31.325
For Velodyne type LIDARs,

155
00:08:31.325 --> 00:08:34.615
the elevation angle is fixed
for a given scan line.

156
00:08:34.615 --> 00:08:37.460
Now, suppose we want to
determine the cartesian

157
00:08:37.460 --> 00:08:41.555
XYZ coordinates of our scanned point
in the sensor frame,

158
00:08:41.555 --> 00:08:43.730
which is something we often
want to do when we're

159
00:08:43.730 --> 00:08:46.790
combining multiple
LIDAR scans into a map.

160
00:08:46.790 --> 00:08:50.345
To convert from spherical
to Cartesian coordinates,

161
00:08:50.345 --> 00:08:52.400
we use the same formulas you would've

162
00:08:52.400 --> 00:08:55.190
encountered in your mechanics classes.

163
00:08:55.190 --> 00:08:58.535
This gives us an inverse sensor model.

164
00:08:58.535 --> 00:09:00.920
We say this is the inverse model because

165
00:09:00.920 --> 00:09:03.950
our actual measurements are
given in spherical coordinates,

166
00:09:03.950 --> 00:09:06.260
and we're trying to
reconstruct the Cartesian

167
00:09:06.260 --> 00:09:09.920
coordinates at the points
that gave rise to them.

168
00:09:09.920 --> 00:09:13.580
Note that we haven't said anything
about measurement noise yet,

169
00:09:13.580 --> 00:09:16.490
we'll come back to that in just a moment.

170
00:09:16.490 --> 00:09:20.975
To go the other way from Cartesian
coordinates to spherical coordinates,

171
00:09:20.975 --> 00:09:24.200
we can work out the inverse
transformation given here.

172
00:09:24.200 --> 00:09:27.275
This is our forward sensor
model for a 3D LIDAR,

173
00:09:27.275 --> 00:09:29.390
which given a set of Cartesian

174
00:09:29.390 --> 00:09:33.110
coordinates defines what
the sensor would actually report.

175
00:09:33.110 --> 00:09:35.990
Now, most of the time the
self-driving cars we're working

176
00:09:35.990 --> 00:09:39.170
with use 3D LIDAR sensors
like the Velodyne,

177
00:09:39.170 --> 00:09:42.845
but sometimes you might want
to use a 2D LIDAR on its own,

178
00:09:42.845 --> 00:09:46.040
whether for detecting obstacles
or for state estimation in

179
00:09:46.040 --> 00:09:49.760
more structured environments
such as parking garages.

180
00:09:49.760 --> 00:09:52.920
Some cars have multiple 2D LIDARs

181
00:09:52.920 --> 00:09:56.410
strategically placed to
act as a single 3D LIDAR,

182
00:09:56.410 --> 00:10:00.325
covering different areas with a greater
or lesser density of measurements.

183
00:10:00.325 --> 00:10:05.365
For 2D LIDARs, we use exactly the same
forward and inverse sensor models.

184
00:10:05.365 --> 00:10:08.200
But the elevation angle
enhance the z component of

185
00:10:08.200 --> 00:10:11.555
the 3D point in the sensor
frame are both zero.

186
00:10:11.555 --> 00:10:13.870
In other words, all of
our measurements are

187
00:10:13.870 --> 00:10:16.390
confined to the XY plane of the sensor,

188
00:10:16.390 --> 00:10:18.670
and our spherical
coordinates collapsed to

189
00:10:18.670 --> 00:10:21.745
the familiar 2D polar coordinates.

190
00:10:21.745 --> 00:10:25.570
We've seen now how to convert between
spherical coordinates measured by

191
00:10:25.570 --> 00:10:27.520
the sensor and the cartesian

192
00:10:27.520 --> 00:10:30.610
coordinates that we'll typically be
interested in for state estimation,

193
00:10:30.610 --> 00:10:32.705
but what about measurement noise?

194
00:10:32.705 --> 00:10:34.475
For LIDAR sensors, there are

195
00:10:34.475 --> 00:10:37.355
several important sources
of noise to consider.

196
00:10:37.355 --> 00:10:39.740
First, there is uncertainty in

197
00:10:39.740 --> 00:10:42.710
the exact time of arrival
of the reflected signal,

198
00:10:42.710 --> 00:10:45.260
which comes from the fact that
the stopwatch we use to compute

199
00:10:45.260 --> 00:10:48.900
the time of flight necessarily
has a limited resolution.

200
00:10:48.900 --> 00:10:51.590
Similarly, there is uncertainty in

201
00:10:51.590 --> 00:10:53.870
the exact orientation
of the mirror in 2D and

202
00:10:53.870 --> 00:10:56.030
3D LIDARs since the encoder

203
00:10:56.030 --> 00:10:59.165
is used to measure
this also have limited resolution.

204
00:10:59.165 --> 00:11:02.090
Another important factor
is the interaction with

205
00:11:02.090 --> 00:11:05.650
the target surface which can
degrade the return signal.

206
00:11:05.650 --> 00:11:08.900
For example, if the surface
is completely black,

207
00:11:08.900 --> 00:11:11.690
it might absorb most of the laser pulse.

208
00:11:11.690 --> 00:11:14.240
Or if it's very shiny like a mirror,

209
00:11:14.240 --> 00:11:15.860
the laser pulse might be scattered

210
00:11:15.860 --> 00:11:19.015
completely away from
the original pulse direction.

211
00:11:19.015 --> 00:11:20.790
In both of these cases,

212
00:11:20.790 --> 00:11:23.870
the LIDAR will typically
report a maximum range error,

213
00:11:23.870 --> 00:11:26.970
which could mean there is empty space
along the beam direction,

214
00:11:26.970 --> 00:11:28.385
or that the pulse encountered

215
00:11:28.385 --> 00:11:31.825
a highly absorptive or
highly reflective surface.

216
00:11:31.825 --> 00:11:35.960
In other words, you simply can't
tell if something is there or not,

217
00:11:35.960 --> 00:11:39.110
and this can be a problem for
safety if your self-driving car is

218
00:11:39.110 --> 00:11:43.450
relying on LIDAR alone to
detect and avoid obstacles.

219
00:11:43.450 --> 00:11:45.980
Finally, the speed of light actually

220
00:11:45.980 --> 00:11:49.055
varies depending on the material
it's traveling through.

221
00:11:49.055 --> 00:11:52.370
The temperature and humidity
of the air can also suddenly

222
00:11:52.370 --> 00:11:56.605
affect the speed of light in our
time-of-flight calculations for example.

223
00:11:56.605 --> 00:11:59.600
These factors are
commonly accounted for by

224
00:11:59.600 --> 00:12:02.360
assuming additive zero-mean
Gaussian noise on

225
00:12:02.360 --> 00:12:04.220
the spherical coordinates with

226
00:12:04.220 --> 00:12:08.215
an empirically determined or
manually tuned covariance.

227
00:12:08.215 --> 00:12:10.245
As we've seen before,

228
00:12:10.245 --> 00:12:12.800
the Gaussian noise model is
particularly convenient for

229
00:12:12.800 --> 00:12:17.330
state estimation even if it's not
perfectly accurate in most cases.

230
00:12:17.330 --> 00:12:20.180
Another very important source
of error that can't be

231
00:12:20.180 --> 00:12:22.970
accounted for so easily
is motion distortion,

232
00:12:22.970 --> 00:12:26.090
which arises because the vehicle
the LIDAR is attached to is

233
00:12:26.090 --> 00:12:29.375
usually moving relative to
the environment it's scanning.

234
00:12:29.375 --> 00:12:31.520
Now, although the car is unlikely to be

235
00:12:31.520 --> 00:12:34.325
moving at an appreciable fraction
of the speed of light,

236
00:12:34.325 --> 00:12:36.320
it is often going to be moving at

237
00:12:36.320 --> 00:12:40.190
an appreciable fraction of
the rotation speed of the sensor itself,

238
00:12:40.190 --> 00:12:43.130
which is typically around 5-20 hertz when

239
00:12:43.130 --> 00:12:46.775
scanning objects at distances
of 10 to a 100 meters.

240
00:12:46.775 --> 00:12:50.390
This means that every single point
in a LIDAR sweep is taken from

241
00:12:50.390 --> 00:12:53.975
a slightly different position and
a slightly different orientation,

242
00:12:53.975 --> 00:12:56.180
and this can cause artifacts such as

243
00:12:56.180 --> 00:12:59.240
duplicate objects to
appear in the LIDAR scans.

244
00:12:59.240 --> 00:13:01.190
This makes it much harder for

245
00:13:01.190 --> 00:13:03.710
a self-driving car to
understand its environment,

246
00:13:03.710 --> 00:13:07.129
and correcting this motion
distortion usually requires

247
00:13:07.129 --> 00:13:09.380
an accurate motion model for the vehicle

248
00:13:09.380 --> 00:13:13.195
provided by GPS and INS for example.

249
00:13:13.195 --> 00:13:17.060
To recap, LIDAR sensors
measure distances by emitting

250
00:13:17.060 --> 00:13:21.055
pulse laser light and measuring
the time of flight of the pulse.

251
00:13:21.055 --> 00:13:25.460
2D or 3D LIDAR is extend
this principle by using a mirror to

252
00:13:25.460 --> 00:13:27.350
sweep the laser across the environment

253
00:13:27.350 --> 00:13:29.630
and measure distances in many directions.

254
00:13:29.630 --> 00:13:31.725
In the next video,

255
00:13:31.725 --> 00:13:35.780
we'll look more closely at the point
clouds created by 2D and 3D LIDARs,

256
00:13:35.780 --> 00:13:40.140
and how we can use them for state
estimation on-board our self-driving car.